#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¨ç‰©å›¾ç‰‡çˆ¬è™«
ç”¨äºæ”¶é›†å„ç§åŠ¨ç‰©å›¾ç‰‡å’ŒåŠ¨å›¾ï¼Œä½œä¸ºä¹¦ç±åˆ›ä½œç´ æ
"""

import os
import requests
import time
import random
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import json
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('animal_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class AnimalImageCrawler:
    """åŠ¨ç‰©å›¾ç‰‡çˆ¬è™«ç±»"""
    
    def __init__(self, base_dir="images"):
        self.base_dir = Path(base_dir)
        self.animals_dir = self.base_dir / "åŠ¨ç‰©"
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # åŠ¨ç‰©åˆ†ç±»å®šä¹‰
        self.animal_categories = {
            "çŒ«ç§‘åŠ¨ç‰©": ["çŒ«", "è€è™", "ç‹®å­", "è±¹å­", "çŒè±¹", "ç¾æ´²è±¹", "å±±çŒ«", "çŒçŒ"],
            "çŠ¬ç§‘åŠ¨ç‰©": ["ç‹—", "ç‹¼", "ç‹ç‹¸", "éƒŠç‹¼", "å°ç‹—", "é‡‘æ¯›", "å“ˆå£«å¥‡", "æŸ´çŠ¬"],
            "é¸Ÿç±»": ["é¸Ÿ", "è€é¹°", "é¹¦é¹‰", "ä¼é¹…", "å­”é›€", "çŒ«å¤´é¹°", "ç‡•å­", "é¸½å­"],
            "æµ·æ´‹åŠ¨ç‰©": ["é²¸é±¼", "æµ·è±š", "é²¨é±¼", "æµ·é¾Ÿ", "ç« é±¼", "æ°´æ¯", "æµ·é©¬", "èƒèŸ¹"],
            "å†œåœºåŠ¨ç‰©": ["ç‰›", "é©¬", "ç¾Š", "çŒª", "é¸¡", "é¸­", "é¹…", "å…”å­"],
            "é‡ç”ŸåŠ¨ç‰©": ["å¤§è±¡", "é•¿é¢ˆé¹¿", "æ²³é©¬", "çŠ€ç‰›", "æ–‘é©¬", "è¢‹é¼ ", "ç†ŠçŒ«", "è€ƒæ‹‰"],
            "å°åŠ¨ç‰©": ["æ¾é¼ ", "åˆºçŒ¬", "ä»“é¼ ", "å…”å­", "å°é¸Ÿ", "å°çŒ«", "å°ç‹—", "å°é¸­"],
            "åŠ¨å›¾ä¸“åŒº": ["åŠ¨ç‰©åŠ¨å›¾", "å¯çˆ±åŠ¨ç‰©gif", "æç¬‘åŠ¨ç‰©", "åŠ¨ç‰©è¡¨æƒ…åŒ…"]
        }
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.create_directories()
        
    def create_directories(self):
        """åˆ›å»ºå›¾ç‰‡å­˜å‚¨ç›®å½•"""
        for category in self.animal_categories.keys():
            directory = self.animals_dir / category
            directory.mkdir(parents=True, exist_ok=True)
            logging.info(f"åˆ›å»ºç›®å½•: {directory}")
    
    def download_image(self, url, filename, save_dir):
        """ä¸‹è½½å•å¼ å›¾ç‰‡æˆ–åŠ¨å›¾"""
        try:
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(random.uniform(1, 3))
            
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡æˆ–åŠ¨å›¾
            content_type = response.headers.get('content-type', '')
            if 'image' not in content_type.lower():
                logging.warning(f"URLä¸æ˜¯å›¾ç‰‡: {url}")
                return False
                
            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            if not filename.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                ext = self.get_image_extension(url, content_type)
                filename = f"{filename}{ext}"
            
            filepath = save_dir / filename
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if filepath.exists():
                logging.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                return True
                
            # ä¿å­˜å›¾ç‰‡
            with open(filepath, 'wb') as f:
                f.write(response.content)
                
            # è®°å½•æ–‡ä»¶å¤§å°ä¿¡æ¯
            file_size = len(response.content)
            file_size_mb = file_size / (1024 * 1024)
            logging.info(f"ä¸‹è½½æˆåŠŸ: {filename} ({file_size_mb:.2f}MB) -> {save_dir}")
            return True
            
        except Exception as e:
            logging.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            return False
    
    def get_image_extension(self, url, content_type):
        """æ ¹æ®URLå’Œcontent-typeç¡®å®šå›¾ç‰‡æ‰©å±•å"""
        # å…ˆå°è¯•ä»URLè·å–æ‰©å±•å
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()
        if path.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
            return os.path.splitext(path)[1]
        
        # æ ¹æ®content-typeç¡®å®šæ‰©å±•å
        if 'jpeg' in content_type:
            return '.jpg'
        elif 'png' in content_type:
            return '.png'
        elif 'gif' in content_type:
            return '.gif'
        elif 'webp' in content_type:
            return '.webp'
        else:
            return '.jpg'  # é»˜è®¤ä½¿ç”¨jpg
    
    def search_baidu_images(self, keyword, max_pages=3, include_gif=False):
        """ç™¾åº¦å›¾ç‰‡æœç´¢"""
        images = []
        base_url = "https://image.baidu.com/search/acjson"
        
        for page in range(max_pages):
            params = {
                'tn': 'resultjson_com',
                'word': keyword,
                'pn': page * 30,
                'rn': 30,
                'ct': 1,
                'ic': 0,
                'lm': -1,
                'nc': 1,
                'ie': 'utf-8',
                'oe': 'utf-8',
                'face': 0,
            }
            
            # å¦‚æœæœç´¢åŠ¨å›¾ï¼Œæ·»åŠ gifå‚æ•°
            if include_gif or 'gif' in keyword.lower() or 'åŠ¨å›¾' in keyword:
                params['f'] = 'gif'
            
            try:
                response = self.session.get(base_url, params=params, timeout=30)
                data = response.json()
                
                if 'data' in data:
                    for item in data['data']:
                        if 'thumbURL' in item and 'middleURL' in item:
                            images.append({
                                'thumb_url': item['thumbURL'],
                                'middle_url': item['middleURL'],
                                'title': item.get('fromPageTitle', ''),
                                'keyword': keyword,
                                'is_gif': include_gif or 'gif' in keyword.lower()
                            })
                
                logging.info(f"ç™¾åº¦æœç´¢ '{keyword}' ç¬¬{page+1}é¡µï¼Œè·å–{len(data.get('data', []))}å¼ å›¾ç‰‡")
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                logging.error(f"ç™¾åº¦æœç´¢å¤±è´¥ {keyword} ç¬¬{page+1}é¡µ: {e}")
                continue
        
        return images
    
    def categorize_animal(self, title, keyword):
        """æ ¹æ®æ ‡é¢˜å’Œå…³é”®è¯å¯¹åŠ¨ç‰©å›¾ç‰‡è¿›è¡Œåˆ†ç±»"""
        title_lower = title.lower()
        keyword_lower = keyword.lower()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåŠ¨å›¾
        if any(gif_word in title_lower or gif_word in keyword_lower 
               for gif_word in ['gif', 'åŠ¨å›¾', 'è¡¨æƒ…åŒ…', 'æç¬‘']):
            return "åŠ¨å›¾ä¸“åŒº"
        
        # æ£€æŸ¥å„ä¸ªåŠ¨ç‰©åˆ†ç±»
        for category, animals in self.animal_categories.items():
            if category == "åŠ¨å›¾ä¸“åŒº":
                continue
            for animal in animals:
                if animal in title_lower or animal in keyword_lower:
                    return category
        
        # é»˜è®¤åˆ†ç±»ä¸ºé‡ç”ŸåŠ¨ç‰©
        return "é‡ç”ŸåŠ¨ç‰©"
    
    def get_search_keywords(self):
        """è·å–æœç´¢å…³é”®è¯åˆ—è¡¨"""
        keywords = []
        
        # ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆæœç´¢å…³é”®è¯
        for category, animals in self.animal_categories.items():
            for animal in animals:
                # åŸºç¡€å…³é”®è¯
                keywords.append(animal)
                # æ·»åŠ "é«˜æ¸…"ã€"å¯çˆ±"ç­‰ä¿®é¥°è¯
                keywords.append(f"{animal} é«˜æ¸…")
                keywords.append(f"å¯çˆ±{animal}")
                
                # ä¸ºåŠ¨å›¾ä¸“åŒºæ·»åŠ gifå…³é”®è¯
                if category == "åŠ¨å›¾ä¸“åŒº" or animal in ["åŠ¨ç‰©åŠ¨å›¾", "å¯çˆ±åŠ¨ç‰©gif"]:
                    keywords.append(f"{animal} gif")
                    keywords.append(f"{animal} åŠ¨å›¾")
        
        # æ·»åŠ ä¸€äº›é€šç”¨çš„åŠ¨ç‰©å…³é”®è¯
        general_keywords = [
            "é‡ç”ŸåŠ¨ç‰©", "åŠ¨ç‰©ä¸–ç•Œ", "å¯çˆ±åŠ¨ç‰©", "åŠ¨ç‰©æ‘„å½±",
            "èŒå® ", "åŠ¨ç‰©å›­", "é‡ç”ŸåŠ¨ç‰©å›­", "åŠ¨ç‰©é«˜æ¸…å£çº¸",
            "åŠ¨ç‰©gif", "æç¬‘åŠ¨ç‰©", "åŠ¨ç‰©è¡¨æƒ…åŒ…"
        ]
        keywords.extend(general_keywords)
        
        return keywords
    
    def run(self, max_images_per_category=50):
        """è¿è¡Œçˆ¬è™«"""
        logging.info("å¼€å§‹çˆ¬å–åŠ¨ç‰©å›¾ç‰‡...")
        
        keywords = self.get_search_keywords()
        random.shuffle(keywords)  # éšæœºæ‰“ä¹±å…³é”®è¯é¡ºåº
        
        total_downloaded = 0
        category_counts = {category: 0 for category in self.animal_categories.keys()}
        
        for keyword in keywords:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æœç´¢åŠ¨å›¾
            include_gif = 'gif' in keyword.lower() or 'åŠ¨å›¾' in keyword
            
            logging.info(f"æœç´¢å…³é”®è¯: {keyword}")
            images = self.search_baidu_images(keyword, max_pages=2, include_gif=include_gif)
            
            for i, img_info in enumerate(images):
                # ç¡®å®šå›¾ç‰‡åˆ†ç±»å’Œä¿å­˜ç›®å½•
                category = self.categorize_animal(img_info['title'], img_info['keyword'])
                
                # æ£€æŸ¥è¯¥åˆ†ç±»æ˜¯å¦å·²è¾¾åˆ°ä¸Šé™
                if category_counts[category] >= max_images_per_category:
                    continue
                
                save_dir = self.animals_dir / category
                
                # ç”Ÿæˆæ–‡ä»¶å
                animal_name = keyword.replace(' ', '_').replace('å¯çˆ±', '').replace('é«˜æ¸…', '')
                filename = f"{animal_name}_{category_counts[category]+1:03d}"
                
                # ä¸‹è½½å›¾ç‰‡
                success = self.download_image(
                    img_info['middle_url'], 
                    filename, 
                    save_dir
                )
                
                if success:
                    total_downloaded += 1
                    category_counts[category] += 1
                
                # æ§åˆ¶æ€»ä¸‹è½½æ•°é‡
                if total_downloaded >= 500:
                    logging.info("å·²ä¸‹è½½500å¼ å›¾ç‰‡ï¼Œåœæ­¢ä¸‹è½½")
                    break
            
            if total_downloaded >= 500:
                break
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†ç±»éƒ½å·²è¾¾åˆ°ä¸Šé™
            if all(count >= max_images_per_category for count in category_counts.values()):
                logging.info("æ‰€æœ‰åˆ†ç±»éƒ½å·²è¾¾åˆ°ä¸‹è½½ä¸Šé™")
                break
        
        logging.info(f"çˆ¬è™«å®Œæˆï¼æ€»å…±ä¸‹è½½äº† {total_downloaded} å¼ å›¾ç‰‡")
        
        # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
        self.generate_report(total_downloaded, category_counts)
    
    def generate_report(self, total_downloaded, category_counts):
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
        report = {
            "æ€»ä¸‹è½½æ•°é‡": total_downloaded,
            "ä¸‹è½½æ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S"),
            "åˆ†ç±»ç»Ÿè®¡": {},
            "åŠ¨ç‰©ç±»åˆ«": list(self.animal_categories.keys())
        }
        
        # ç»Ÿè®¡å„åˆ†ç±»çš„å®é™…å›¾ç‰‡æ•°é‡
        for category_dir in self.animals_dir.iterdir():
            if category_dir.is_dir():
                # ç»Ÿè®¡å„ç§ç±»å‹çš„æ–‡ä»¶
                jpg_count = len(list(category_dir.glob("*.jpg")))
                png_count = len(list(category_dir.glob("*.png")))
                gif_count = len(list(category_dir.glob("*.gif")))
                webp_count = len(list(category_dir.glob("*.webp")))
                total_count = jpg_count + png_count + gif_count + webp_count
                
                report["åˆ†ç±»ç»Ÿè®¡"][category_dir.name] = {
                    "æ€»æ•°": total_count,
                    "JPG": jpg_count,
                    "PNG": png_count,
                    "GIF": gif_count,
                    "WEBP": webp_count
                }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.animals_dir / "ä¸‹è½½æŠ¥å‘Š.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logging.info(f"ä¸‹è½½æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        print("\n" + "="*60)
        print("ğŸ¾ åŠ¨ç‰©å›¾ç‰‡ä¸‹è½½å®Œæˆ! ğŸ¾")
        print(f"æ€»å…±ä¸‹è½½: {total_downloaded} å¼ å›¾ç‰‡")
        print("\nåˆ†ç±»ç»Ÿè®¡:")
        for category, stats in report["åˆ†ç±»ç»Ÿè®¡"].items():
            if stats["æ€»æ•°"] > 0:
                print(f"  ğŸ“ {category}: {stats['æ€»æ•°']} å¼ ")
                if stats["GIF"] > 0:
                    print(f"     â””â”€ åŒ…å« {stats['GIF']} å¼ åŠ¨å›¾")
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¾ åŠ¨ç‰©å›¾ç‰‡çˆ¬è™« ğŸ¾")
    print("ç”¨äºæ”¶é›†å„ç§åŠ¨ç‰©å›¾ç‰‡å’ŒåŠ¨å›¾ä½œä¸ºä¹¦ç±åˆ›ä½œç´ æ")
    print("-" * 50)
    
    # è¯¢é—®ç”¨æˆ·é…ç½®
    print("\né…ç½®é€‰é¡¹:")
    print("1. æ¯ä¸ªåˆ†ç±»ä¸‹è½½å¤šå°‘å¼ å›¾ç‰‡? (é»˜è®¤: 50)")
    max_images = input("è¯·è¾“å…¥æ•°å­— (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤å€¼): ").strip()
    
    try:
        max_images_per_category = int(max_images) if max_images else 50
    except ValueError:
        max_images_per_category = 50
        print("è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼: 50")
    
    print(f"\nå°†ä¸ºæ¯ä¸ªåŠ¨ç‰©åˆ†ç±»ä¸‹è½½æœ€å¤š {max_images_per_category} å¼ å›¾ç‰‡")
    print("å¼€å§‹ä¸‹è½½...\n")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶è¿è¡Œ
    crawler = AnimalImageCrawler()
    
    try:
        crawler.run(max_images_per_category=max_images_per_category)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        logging.error(f"ç¨‹åºå¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()