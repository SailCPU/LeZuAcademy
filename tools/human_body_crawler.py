#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äººä½“å™¨å®˜ä¸ç»†èƒå›¾ç‰‡çˆ¬è™«
ç”¨äºæ”¶é›†äººä½“å™¨å®˜ã€ç»†èƒç­‰åŒ»å­¦ç›¸å…³å›¾ç‰‡ï¼Œä½œä¸ºä¹¦ç±åˆ›ä½œç´ æ
"""

import os
import requests
import time
import random
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import json
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('human_body_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class HumanBodyCrawler:
    """äººä½“å™¨å®˜ä¸ç»†èƒå›¾ç‰‡çˆ¬è™«ç±»"""
    
    def __init__(self, base_dir="images"):
        self.base_dir = Path(base_dir)
        self.human_body_dir = self.base_dir / "äººä½“å™¨å®˜ä¸ç»†èƒ"
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # äººä½“å™¨å®˜ä¸ç»†èƒåˆ†ç±»å®šä¹‰
        self.body_categories = {
            "å¿ƒè¡€ç®¡ç³»ç»Ÿ": [
                "å¿ƒè„", "è¡€ç®¡", "åŠ¨è„‰", "é™è„‰", "æ¯›ç»†è¡€ç®¡", "å¿ƒè„è§£å‰–", "å¿ƒè„ç»“æ„",
                "è¡€æ¶²å¾ªç¯", "å¿ƒè‚Œ", "å¿ƒæˆ¿", "å¿ƒå®¤", "ä¸»åŠ¨è„‰", "è‚ºåŠ¨è„‰"
            ],
            "å‘¼å¸ç³»ç»Ÿ": [
                "è‚º", "æ°”ç®¡", "æ”¯æ°”ç®¡", "è‚ºæ³¡", "é¼»è…”", "å’½å–‰", "å–‰å’™", 
                "å‘¼å¸é“", "è‚ºéƒ¨ç»“æ„", "æ°”ä½“äº¤æ¢", "è‚ºå¶", "èƒ¸è…”"
            ],
            "æ¶ˆåŒ–ç³»ç»Ÿ": [
                "èƒƒ", "è‚è„", "è‚ é“", "å°è‚ ", "å¤§è‚ ", "é£Ÿé“", "èƒ°è…º", "èƒ†å›Š",
                "åäºŒæŒ‡è‚ ", "ç»“è‚ ", "ç›´è‚ ", "æ¶ˆåŒ–é“", "èƒƒå£", "è‚ ç»’æ¯›"
            ],
            "ç¥ç»ç³»ç»Ÿ": [
                "å¤§è„‘", "è„Šé«“", "ç¥ç»", "ç¥ç»å…ƒ", "å¤§è„‘çš®å±‚", "å°è„‘", "è„‘å¹²",
                "ç¥ç»ç»†èƒ", "çªè§¦", "è„‘éƒ¨ç»“æ„", "ä¸­æ¢ç¥ç»", "å‘¨å›´ç¥ç»"
            ],
            "å†…åˆ†æ³Œç³»ç»Ÿ": [
                "ç”²çŠ¶è…º", "è‚¾ä¸Šè…º", "èƒ°å²›", "å‚ä½“", "ä¸‹ä¸˜è„‘", "æ€§è…º",
                "å†…åˆ†æ³Œè…º", "æ¿€ç´ ", "èƒ°å²›ç´ ", "ç”²çŠ¶è…ºæ¿€ç´ "
            ],
            "æ³Œå°¿ç³»ç»Ÿ": [
                "è‚¾è„", "è†€èƒ±", "è¾“å°¿ç®¡", "å°¿é“", "è‚¾å•ä½", "è‚¾å°çƒ",
                "è‚¾å°ç®¡", "æ³Œå°¿é“", "è‚¾è„ç»“æ„", "æ’æ³„ç³»ç»Ÿ"
            ],
            "éª¨éª¼è‚Œè‚‰ç³»ç»Ÿ": [
                "éª¨éª¼", "è‚Œè‚‰", "å…³èŠ‚", "éª¨å¤´", "è‚Œçº¤ç»´", "éª¨éª¼ç»“æ„",
                "è‚Œè‚‰ç»„ç»‡", "éª¨ç»†èƒ", "è½¯éª¨", "éŸ§å¸¦", "è‚Œè…±"
            ],
            "ç»†èƒç±»å‹": [
                "ç»†èƒ", "çº¢ç»†èƒ", "ç™½ç»†èƒ", "è¡€å°æ¿", "ç¥ç»ç»†èƒ", "è‚Œç»†èƒ",
                "ä¸Šçš®ç»†èƒ", "å¹²ç»†èƒ", "ç™Œç»†èƒ", "ç»†èƒåˆ†è£‚", "ç»†èƒè†œ", "ç»†èƒæ ¸",
                "çº¿ç²’ä½“", "ç»†èƒå™¨", "DNA", "æŸ“è‰²ä½“"
            ],
            "ç»„ç»‡å­¦": [
                "ç»„ç»‡", "ä¸Šçš®ç»„ç»‡", "ç»“ç¼”ç»„ç»‡", "è‚Œè‚‰ç»„ç»‡", "ç¥ç»ç»„ç»‡",
                "è¡€æ¶²ç»„ç»‡", "æ·‹å·´ç»„ç»‡", "è„‚è‚ªç»„ç»‡", "çº¤ç»´ç»„ç»‡"
            ],
            "åŒ»å­¦å½±åƒ": [
                "Xå…‰", "CTæ‰«æ", "MRI", "è¶…å£°æ³¢", "åŒ»å­¦å½±åƒ", "è§£å‰–å›¾",
                "äººä½“ç»“æ„å›¾", "å™¨å®˜åˆ‡ç‰‡", "ç»„ç»‡åˆ‡ç‰‡", "ç—…ç†å›¾ç‰‡"
            ]
        }
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.create_directories()
        
    def create_directories(self):
        """åˆ›å»ºå›¾ç‰‡å­˜å‚¨ç›®å½•"""
        for category in self.body_categories.keys():
            directory = self.human_body_dir / category
            directory.mkdir(parents=True, exist_ok=True)
            logging.info(f"åˆ›å»ºç›®å½•: {directory}")
    
    def download_image(self, url, filename, save_dir):
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(random.uniform(1, 3))
            
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡
            content_type = response.headers.get('content-type', '')
            if 'image' not in content_type.lower():
                logging.warning(f"URLä¸æ˜¯å›¾ç‰‡: {url}")
                return False
                
            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            if not filename.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                ext = self.get_image_extension(url, content_type)
                filename = f"{filename}{ext}"
            
            filepath = save_dir / filename
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if filepath.exists():
                logging.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                return True
                
            # ä¿å­˜å›¾ç‰‡
            with open(filepath, 'wb') as f:
                f.write(response.content)
                
            # è®°å½•æ–‡ä»¶å¤§å°ä¿¡æ¯
            file_size = len(response.content)
            file_size_mb = file_size / (1024 * 1024)
            logging.info(f"ä¸‹è½½æˆåŠŸ: {filename} ({file_size_mb:.2f}MB) -> {save_dir}")
            return True
            
        except Exception as e:
            logging.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            return False
    
    def get_image_extension(self, url, content_type):
        """æ ¹æ®URLå’Œcontent-typeç¡®å®šå›¾ç‰‡æ‰©å±•å"""
        # å…ˆå°è¯•ä»URLè·å–æ‰©å±•å
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()
        if path.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
            return os.path.splitext(path)[1]
        
        # æ ¹æ®content-typeç¡®å®šæ‰©å±•å
        if 'jpeg' in content_type:
            return '.jpg'
        elif 'png' in content_type:
            return '.png'
        elif 'gif' in content_type:
            return '.gif'
        elif 'webp' in content_type:
            return '.webp'
        else:
            return '.jpg'  # é»˜è®¤ä½¿ç”¨jpg
    
    def search_baidu_images(self, keyword, max_pages=3):
        """ç™¾åº¦å›¾ç‰‡æœç´¢"""
        images = []
        base_url = "https://image.baidu.com/search/acjson"
        
        for page in range(max_pages):
            params = {
                'tn': 'resultjson_com',
                'word': keyword,
                'pn': page * 30,
                'rn': 30,
                'ct': 1,
                'ic': 0,
                'lm': -1,
                'nc': 1,
                'ie': 'utf-8',
                'oe': 'utf-8',
                'face': 0,
            }
            
            try:
                response = self.session.get(base_url, params=params, timeout=30)
                data = response.json()
                
                if 'data' in data:
                    for item in data['data']:
                        if 'thumbURL' in item and 'middleURL' in item:
                            images.append({
                                'thumb_url': item['thumbURL'],
                                'middle_url': item['middleURL'],
                                'title': item.get('fromPageTitle', ''),
                                'keyword': keyword
                            })
                
                logging.info(f"ç™¾åº¦æœç´¢ '{keyword}' ç¬¬{page+1}é¡µï¼Œè·å–{len(data.get('data', []))}å¼ å›¾ç‰‡")
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                logging.error(f"ç™¾åº¦æœç´¢å¤±è´¥ {keyword} ç¬¬{page+1}é¡µ: {e}")
                continue
        
        return images
    
    def categorize_body_part(self, title, keyword):
        """æ ¹æ®æ ‡é¢˜å’Œå…³é”®è¯å¯¹äººä½“å™¨å®˜å›¾ç‰‡è¿›è¡Œåˆ†ç±»"""
        title_lower = title.lower()
        keyword_lower = keyword.lower()
        
        # æ£€æŸ¥å„ä¸ªäººä½“ç³»ç»Ÿåˆ†ç±»
        for category, parts in self.body_categories.items():
            for part in parts:
                if part in title_lower or part in keyword_lower:
                    return category
        
        # å¦‚æœåŒ…å«åŒ»å­¦ã€è§£å‰–ç­‰é€šç”¨è¯æ±‡ï¼Œå½’ç±»åˆ°åŒ»å­¦å½±åƒ
        medical_terms = ['åŒ»å­¦', 'è§£å‰–', 'ç”Ÿç‰©', 'ç”Ÿç†', 'ç—…ç†', 'ä¸´åºŠ']
        if any(term in title_lower or term in keyword_lower for term in medical_terms):
            return "åŒ»å­¦å½±åƒ"
        
        # é»˜è®¤åˆ†ç±»ä¸ºåŒ»å­¦å½±åƒ
        return "åŒ»å­¦å½±åƒ"
    
    def get_search_keywords(self):
        """è·å–æœç´¢å…³é”®è¯åˆ—è¡¨"""
        keywords = []
        
        # ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆæœç´¢å…³é”®è¯
        for category, parts in self.body_categories.items():
            for part in parts:
                # åŸºç¡€å…³é”®è¯
                keywords.append(part)
                # æ·»åŠ "è§£å‰–"ã€"ç»“æ„"ç­‰ä¿®é¥°è¯
                keywords.append(f"{part} è§£å‰–")
                keywords.append(f"{part} ç»“æ„")
                keywords.append(f"{part} åŒ»å­¦")
                
                # ä¸ºç»†èƒç›¸å…³æ·»åŠ æ˜¾å¾®é•œå…³é”®è¯
                if "ç»†èƒ" in part:
                    keywords.append(f"{part} æ˜¾å¾®é•œ")
                    keywords.append(f"{part} ç”µé•œ")
        
        # æ·»åŠ ä¸€äº›é€šç”¨çš„åŒ»å­¦å…³é”®è¯
        general_keywords = [
            "äººä½“è§£å‰–", "äººä½“ç»“æ„", "åŒ»å­¦å›¾è°±", "è§£å‰–å­¦",
            "ç”Ÿç†å­¦", "ç»„ç»‡å­¦", "ç»†èƒç”Ÿç‰©å­¦", "äººä½“å™¨å®˜",
            "åŒ»å­¦æ’å›¾", "è§£å‰–å›¾", "äººä½“ç³»ç»Ÿ", "ç”Ÿç‰©åŒ»å­¦",
            "ä¸´åºŠè§£å‰–", "ç—…ç†è§£å‰–", "åŠŸèƒ½è§£å‰–"
        ]
        keywords.extend(general_keywords)
        
        return keywords
    
    def run(self, max_images_per_category=30):
        """è¿è¡Œçˆ¬è™«"""
        logging.info("å¼€å§‹çˆ¬å–äººä½“å™¨å®˜ä¸ç»†èƒå›¾ç‰‡...")
        
        keywords = self.get_search_keywords()
        random.shuffle(keywords)  # éšæœºæ‰“ä¹±å…³é”®è¯é¡ºåº
        
        total_downloaded = 0
        category_counts = {category: 0 for category in self.body_categories.keys()}
        
        for keyword in keywords:
            logging.info(f"æœç´¢å…³é”®è¯: {keyword}")
            images = self.search_baidu_images(keyword, max_pages=2)
            
            for i, img_info in enumerate(images):
                # ç¡®å®šå›¾ç‰‡åˆ†ç±»å’Œä¿å­˜ç›®å½•
                category = self.categorize_body_part(img_info['title'], img_info['keyword'])
                
                # æ£€æŸ¥è¯¥åˆ†ç±»æ˜¯å¦å·²è¾¾åˆ°ä¸Šé™
                if category_counts[category] >= max_images_per_category:
                    continue
                
                save_dir = self.human_body_dir / category
                
                # ç”Ÿæˆæ–‡ä»¶å
                clean_keyword = keyword.replace(' ', '_').replace('è§£å‰–', '').replace('ç»“æ„', '').replace('åŒ»å­¦', '')
                filename = f"{clean_keyword}_{category_counts[category]+1:03d}"
                
                # ä¸‹è½½å›¾ç‰‡
                success = self.download_image(
                    img_info['middle_url'], 
                    filename, 
                    save_dir
                )
                
                if success:
                    total_downloaded += 1
                    category_counts[category] += 1
                
                # æ§åˆ¶æ€»ä¸‹è½½æ•°é‡
                if total_downloaded >= 300:
                    logging.info("å·²ä¸‹è½½300å¼ å›¾ç‰‡ï¼Œåœæ­¢ä¸‹è½½")
                    break
            
            if total_downloaded >= 300:
                break
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†ç±»éƒ½å·²è¾¾åˆ°ä¸Šé™
            if all(count >= max_images_per_category for count in category_counts.values()):
                logging.info("æ‰€æœ‰åˆ†ç±»éƒ½å·²è¾¾åˆ°ä¸‹è½½ä¸Šé™")
                break
        
        logging.info(f"çˆ¬è™«å®Œæˆï¼æ€»å…±ä¸‹è½½äº† {total_downloaded} å¼ å›¾ç‰‡")
        
        # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
        self.generate_report(total_downloaded, category_counts)
    
    def generate_report(self, total_downloaded, category_counts):
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
        report = {
            "æ€»ä¸‹è½½æ•°é‡": total_downloaded,
            "ä¸‹è½½æ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S"),
            "åˆ†ç±»ç»Ÿè®¡": {},
            "äººä½“ç³»ç»Ÿ": list(self.body_categories.keys())
        }
        
        # ç»Ÿè®¡å„åˆ†ç±»çš„å®é™…å›¾ç‰‡æ•°é‡
        for category_dir in self.human_body_dir.iterdir():
            if category_dir.is_dir():
                # ç»Ÿè®¡å„ç§ç±»å‹çš„æ–‡ä»¶
                jpg_count = len(list(category_dir.glob("*.jpg")))
                png_count = len(list(category_dir.glob("*.png")))
                gif_count = len(list(category_dir.glob("*.gif")))
                webp_count = len(list(category_dir.glob("*.webp")))
                total_count = jpg_count + png_count + gif_count + webp_count
                
                report["åˆ†ç±»ç»Ÿè®¡"][category_dir.name] = {
                    "æ€»æ•°": total_count,
                    "JPG": jpg_count,
                    "PNG": png_count,
                    "GIF": gif_count,
                    "WEBP": webp_count
                }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.human_body_dir / "ä¸‹è½½æŠ¥å‘Š.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logging.info(f"ä¸‹è½½æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        print("\n" + "="*60)
        print("ğŸ§¬ äººä½“å™¨å®˜ä¸ç»†èƒå›¾ç‰‡ä¸‹è½½å®Œæˆ! ğŸ§¬")
        print(f"æ€»å…±ä¸‹è½½: {total_downloaded} å¼ å›¾ç‰‡")
        print("\nåˆ†ç±»ç»Ÿè®¡:")
        for category, stats in report["åˆ†ç±»ç»Ÿè®¡"].items():
            if stats["æ€»æ•°"] > 0:
                print(f"  ğŸ“ {category}: {stats['æ€»æ•°']} å¼ ")
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¬ äººä½“å™¨å®˜ä¸ç»†èƒå›¾ç‰‡çˆ¬è™« ğŸ§¬")
    print("ç”¨äºæ”¶é›†äººä½“å™¨å®˜ã€ç»†èƒç­‰åŒ»å­¦å›¾ç‰‡ä½œä¸ºä¹¦ç±åˆ›ä½œç´ æ")
    print("-" * 50)
    
    # è¯¢é—®ç”¨æˆ·é…ç½®
    print("\né…ç½®é€‰é¡¹:")
    print("1. æ¯ä¸ªåˆ†ç±»ä¸‹è½½å¤šå°‘å¼ å›¾ç‰‡? (é»˜è®¤: 30)")
    max_images = input("è¯·è¾“å…¥æ•°å­— (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤å€¼): ").strip()
    
    try:
        max_images_per_category = int(max_images) if max_images else 30
    except ValueError:
        max_images_per_category = 30
        print("è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼: 30")
    
    print(f"\nå°†ä¸ºæ¯ä¸ªäººä½“ç³»ç»Ÿåˆ†ç±»ä¸‹è½½æœ€å¤š {max_images_per_category} å¼ å›¾ç‰‡")
    print("å¼€å§‹ä¸‹è½½...\n")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶è¿è¡Œ
    crawler = HumanBodyCrawler()
    
    try:
        crawler.run(max_images_per_category=max_images_per_category)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        logging.error(f"ç¨‹åºå¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()