#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äººä½“ç»†èƒå›¾ç‰‡çˆ¬è™«
ä¸“é—¨çˆ¬å–å„ç§äººä½“ç»†èƒçš„é«˜è´¨é‡å›¾ç‰‡ï¼Œä½¿ç”¨è‹±æ–‡å…³é”®è¯ä»å›½å¤–ç½‘ç«™è·å–
"""

import os
import requests
import time
import random
from urllib.parse import urljoin, urlparse, quote
from bs4 import BeautifulSoup
import json
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('cell_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class CellImageCrawler:
    """äººä½“ç»†èƒå›¾ç‰‡çˆ¬è™«ç±»"""
    
    def __init__(self, base_dir="images"):
        self.base_dir = Path(base_dir)
        self.cells_dir = self.base_dir / "äººä½“ç»†èƒ"
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
        })
        
        # äººä½“ç»†èƒåˆ†ç±»å®šä¹‰ï¼ˆè‹±æ–‡å…³é”®è¯ï¼‰
        self.cell_categories = {
            "è¡€æ¶²ç»†èƒ": [
                "red blood cells", "erythrocytes", "white blood cells", "leukocytes",
                "platelets", "thrombocytes", "neutrophils", "lymphocytes", "monocytes",
                "eosinophils", "basophils", "plasma cells", "macrophages"
            ],
            "ç¥ç»ç»†èƒ": [
                "neurons", "nerve cells", "glial cells", "astrocytes", "oligodendrocytes",
                "microglia", "schwann cells", "motor neurons", "sensory neurons",
                "interneurons", "pyramidal cells", "purkinje cells"
            ],
            "è‚Œè‚‰ç»†èƒ": [
                "muscle cells", "myocytes", "skeletal muscle cells", "cardiac muscle cells",
                "smooth muscle cells", "muscle fibers", "myofibrils", "cardiomyocytes",
                "satellite cells"
            ],
            "ä¸Šçš®ç»†èƒ": [
                "epithelial cells", "squamous epithelium", "cuboidal epithelium",
                "columnar epithelium", "ciliated epithelium", "keratinocytes",
                "melanocytes", "goblet cells"
            ],
            "ç»“ç¼”ç»„ç»‡ç»†èƒ": [
                "fibroblasts", "chondrocytes", "osteoblasts", "osteocytes", "osteoclasts",
                "adipocytes", "fat cells", "cartilage cells", "bone cells"
            ],
            "å…ç–«ç»†èƒ": [
                "T cells", "B cells", "NK cells", "dendritic cells", "helper T cells",
                "cytotoxic T cells", "regulatory T cells", "memory cells"
            ],
            "å¹²ç»†èƒ": [
                "stem cells", "embryonic stem cells", "adult stem cells", "mesenchymal stem cells",
                "hematopoietic stem cells", "neural stem cells", "induced pluripotent stem cells"
            ],
            "ç”Ÿæ®–ç»†èƒ": [
                "sperm cells", "egg cells", "oocytes", "spermatozoa", "gametes",
                "follicle cells", "granulosa cells"
            ],
            "æ¶ˆåŒ–ç³»ç»Ÿç»†èƒ": [
                "hepatocytes", "liver cells", "pancreatic cells", "gastric cells",
                "intestinal cells", "enterocytes", "parietal cells", "chief cells"
            ],
            "è‚¾è„ç»†èƒ": [
                "kidney cells", "nephron cells", "glomerular cells", "tubular cells",
                "podocytes", "mesangial cells"
            ],
            "ç™Œç»†èƒ": [
                "cancer cells", "tumor cells", "malignant cells", "carcinoma cells",
                "adenocarcinoma", "sarcoma cells", "leukemia cells"
            ],
            "ç»†èƒç»“æ„": [
                "cell nucleus", "mitochondria", "ribosomes", "endoplasmic reticulum",
                "golgi apparatus", "lysosomes", "cell membrane", "cytoplasm",
                "chromosomes", "DNA", "cell organelles"
            ]
        }
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.create_directories()
        
    def create_directories(self):
        """åˆ›å»ºå›¾ç‰‡å­˜å‚¨ç›®å½•"""
        for category in self.cell_categories.keys():
            directory = self.cells_dir / category
            directory.mkdir(parents=True, exist_ok=True)
            logging.info(f"åˆ›å»ºç›®å½•: {directory}")
    
    def download_image(self, url, filename, save_dir):
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(random.uniform(2, 4))
            
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡
            content_type = response.headers.get('content-type', '')
            if 'image' not in content_type.lower():
                logging.warning(f"URLä¸æ˜¯å›¾ç‰‡: {url}")
                return False
                
            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            if not filename.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                ext = self.get_image_extension(url, content_type)
                filename = f"{filename}{ext}"
            
            filepath = save_dir / filename
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if filepath.exists():
                logging.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                return True
                
            # ä¿å­˜å›¾ç‰‡
            with open(filepath, 'wb') as f:
                f.write(response.content)
                
            # è®°å½•æ–‡ä»¶å¤§å°ä¿¡æ¯
            file_size = len(response.content)
            file_size_mb = file_size / (1024 * 1024)
            logging.info(f"ä¸‹è½½æˆåŠŸ: {filename} ({file_size_mb:.2f}MB) -> {save_dir}")
            return True
            
        except Exception as e:
            logging.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            return False
    
    def get_image_extension(self, url, content_type):
        """æ ¹æ®URLå’Œcontent-typeç¡®å®šå›¾ç‰‡æ‰©å±•å"""
        # å…ˆå°è¯•ä»URLè·å–æ‰©å±•å
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()
        if path.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
            return os.path.splitext(path)[1]
        
        # æ ¹æ®content-typeç¡®å®šæ‰©å±•å
        if 'jpeg' in content_type:
            return '.jpg'
        elif 'png' in content_type:
            return '.png'
        elif 'gif' in content_type:
            return '.gif'
        elif 'webp' in content_type:
            return '.webp'
        else:
            return '.jpg'  # é»˜è®¤ä½¿ç”¨jpg
    
    def search_bing_images(self, keyword, max_pages=3):
        """Bingå›¾ç‰‡æœç´¢"""
        images = []
        
        for page in range(max_pages):
            # Bingå›¾ç‰‡æœç´¢URL
            params = {
                'q': keyword,
                'first': page * 20 + 1,
                'count': 20,
                'mkt': 'en-US'
            }
            
            search_url = f"https://www.bing.com/images/search?q={quote(keyword)}&first={page * 20 + 1}&count=20&mkt=en-US"
            
            try:
                response = self.session.get(search_url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾å›¾ç‰‡é“¾æ¥
                img_elements = soup.find_all('img', {'class': 'mimg'})
                
                for img in img_elements:
                    src = img.get('src')
                    if src and src.startswith('http'):
                        images.append({
                            'url': src,
                            'title': img.get('alt', ''),
                            'keyword': keyword
                        })
                
                logging.info(f"Bingæœç´¢ '{keyword}' ç¬¬{page+1}é¡µï¼Œè·å–{len(img_elements)}å¼ å›¾ç‰‡")
                time.sleep(random.uniform(3, 5))
                
            except Exception as e:
                logging.error(f"Bingæœç´¢å¤±è´¥ {keyword} ç¬¬{page+1}é¡µ: {e}")
                continue
        
        return images
    
    def search_duckduckgo_images(self, keyword, max_results=30):
        """DuckDuckGoå›¾ç‰‡æœç´¢"""
        images = []
        
        try:
            # DuckDuckGoå›¾ç‰‡æœç´¢API
            search_url = f"https://duckduckgo.com/"
            
            # é¦–å…ˆè·å–æœç´¢token
            response = self.session.get(search_url)
            
            # ç„¶åè¿›è¡Œå›¾ç‰‡æœç´¢
            search_params = {
                'q': keyword,
                'iax': 'images',
                'ia': 'images'
            }
            
            response = self.session.get(search_url, params=search_params)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç®€åŒ–çš„å›¾ç‰‡æå–ï¼ˆå®é™…çš„DuckDuckGo APIæ›´å¤æ‚ï¼‰
            img_elements = soup.find_all('img')[:max_results]
            
            for img in img_elements:
                src = img.get('src')
                if src and 'http' in src:
                    images.append({
                        'url': src,
                        'title': img.get('alt', ''),
                        'keyword': keyword
                    })
            
            logging.info(f"DuckDuckGoæœç´¢ '{keyword}'ï¼Œè·å–{len(images)}å¼ å›¾ç‰‡")
            time.sleep(random.uniform(2, 4))
            
        except Exception as e:
            logging.error(f"DuckDuckGoæœç´¢å¤±è´¥ {keyword}: {e}")
        
        return images
    
    def search_unsplash_images(self, keyword, max_results=20):
        """Unsplashå›¾ç‰‡æœç´¢ï¼ˆä½¿ç”¨å…¬å¼€APIï¼‰"""
        images = []
        
        try:
            # Unsplashå…¬å¼€æœç´¢URL
            search_url = f"https://unsplash.com/napi/search/photos"
            params = {
                'query': keyword,
                'per_page': max_results
            }
            
            response = self.session.get(search_url, params=params)
            
            if response.status_code == 200:
                data = response.json()
                
                for item in data.get('results', []):
                    if 'urls' in item and 'regular' in item['urls']:
                        images.append({
                            'url': item['urls']['regular'],
                            'title': item.get('alt_description', ''),
                            'keyword': keyword
                        })
                
                logging.info(f"Unsplashæœç´¢ '{keyword}'ï¼Œè·å–{len(images)}å¼ å›¾ç‰‡")
            
            time.sleep(random.uniform(2, 3))
            
        except Exception as e:
            logging.error(f"Unsplashæœç´¢å¤±è´¥ {keyword}: {e}")
        
        return images
    
    def categorize_cell(self, title, keyword):
        """æ ¹æ®æ ‡é¢˜å’Œå…³é”®è¯å¯¹ç»†èƒå›¾ç‰‡è¿›è¡Œåˆ†ç±»"""
        title_lower = title.lower()
        keyword_lower = keyword.lower()
        
        # æ£€æŸ¥å„ä¸ªç»†èƒåˆ†ç±»
        for category, cell_types in self.cell_categories.items():
            for cell_type in cell_types:
                if cell_type.lower() in title_lower or cell_type.lower() in keyword_lower:
                    return category
        
        # é»˜è®¤åˆ†ç±»ä¸ºç»†èƒç»“æ„
        return "ç»†èƒç»“æ„"
    
    def get_search_keywords(self):
        """è·å–æœç´¢å…³é”®è¯åˆ—è¡¨"""
        keywords = []
        
        # ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆæœç´¢å…³é”®è¯
        for category, cell_types in self.cell_categories.items():
            for cell_type in cell_types:
                # åŸºç¡€å…³é”®è¯
                keywords.append(cell_type)
                # æ·»åŠ "microscopy"ã€"histology"ç­‰ä¿®é¥°è¯
                keywords.append(f"{cell_type} microscopy")
                keywords.append(f"{cell_type} histology")
                keywords.append(f"{cell_type} anatomy")
                
                # ä¸ºç‰¹å®šç»†èƒç±»å‹æ·»åŠ ç‰¹æ®Šå…³é”®è¯
                if "cells" in cell_type:
                    keywords.append(f"{cell_type} structure")
                    keywords.append(f"{cell_type} function")
        
        # æ·»åŠ ä¸€äº›é€šç”¨çš„ç»†èƒå­¦å…³é”®è¯
        general_keywords = [
            "human cells", "cell biology", "cell structure", "cell types",
            "cellular anatomy", "histological sections", "cell microscopy",
            "human histology", "cell morphology", "cellular organelles"
        ]
        keywords.extend(general_keywords)
        
        return keywords
    
    def run(self, max_images_per_category=25):
        """è¿è¡Œçˆ¬è™«"""
        logging.info("å¼€å§‹çˆ¬å–äººä½“ç»†èƒå›¾ç‰‡...")
        
        keywords = self.get_search_keywords()
        random.shuffle(keywords)  # éšæœºæ‰“ä¹±å…³é”®è¯é¡ºåº
        
        total_downloaded = 0
        category_counts = {category: 0 for category in self.cell_categories.keys()}
        
        for keyword in keywords[:50]:  # é™åˆ¶å…³é”®è¯æ•°é‡ï¼Œé¿å…è¿‡å¤š
            if total_downloaded >= 300:
                break
                
            logging.info(f"æœç´¢å…³é”®è¯: {keyword}")
            
            # ä½¿ç”¨å¤šä¸ªæœç´¢æº
            all_images = []
            
            # Bingæœç´¢
            try:
                bing_images = self.search_bing_images(keyword, max_pages=2)
                all_images.extend(bing_images)
            except Exception as e:
                logging.error(f"Bingæœç´¢å‡ºé”™: {e}")
            
            # Unsplashæœç´¢
            try:
                unsplash_images = self.search_unsplash_images(keyword, max_results=10)
                all_images.extend(unsplash_images)
            except Exception as e:
                logging.error(f"Unsplashæœç´¢å‡ºé”™: {e}")
            
            # å¤„ç†æœç´¢ç»“æœ
            for i, img_info in enumerate(all_images[:15]):  # æ¯ä¸ªå…³é”®è¯æœ€å¤š15å¼ å›¾
                # ç¡®å®šå›¾ç‰‡åˆ†ç±»å’Œä¿å­˜ç›®å½•
                category = self.categorize_cell(img_info.get('title', ''), img_info['keyword'])
                
                # æ£€æŸ¥è¯¥åˆ†ç±»æ˜¯å¦å·²è¾¾åˆ°ä¸Šé™
                if category_counts[category] >= max_images_per_category:
                    continue
                
                save_dir = self.cells_dir / category
                
                # ç”Ÿæˆæ–‡ä»¶å
                clean_keyword = keyword.replace(' ', '_').replace('microscopy', '').replace('histology', '').replace('anatomy', '')
                filename = f"{clean_keyword}_{category_counts[category]+1:03d}"
                
                # ä¸‹è½½å›¾ç‰‡
                success = self.download_image(
                    img_info['url'], 
                    filename, 
                    save_dir
                )
                
                if success:
                    total_downloaded += 1
                    category_counts[category] += 1
                
                # æ§åˆ¶æ€»ä¸‹è½½æ•°é‡
                if total_downloaded >= 300:
                    logging.info("å·²ä¸‹è½½300å¼ å›¾ç‰‡ï¼Œåœæ­¢ä¸‹è½½")
                    break
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†ç±»éƒ½å·²è¾¾åˆ°ä¸Šé™
            if all(count >= max_images_per_category for count in category_counts.values()):
                logging.info("æ‰€æœ‰åˆ†ç±»éƒ½å·²è¾¾åˆ°ä¸‹è½½ä¸Šé™")
                break
        
        logging.info(f"çˆ¬è™«å®Œæˆï¼æ€»å…±ä¸‹è½½äº† {total_downloaded} å¼ å›¾ç‰‡")
        
        # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
        self.generate_report(total_downloaded, category_counts)
    
    def generate_report(self, total_downloaded, category_counts):
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
        report = {
            "æ€»ä¸‹è½½æ•°é‡": total_downloaded,
            "ä¸‹è½½æ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S"),
            "åˆ†ç±»ç»Ÿè®¡": {},
            "ç»†èƒç±»å‹": list(self.cell_categories.keys())
        }
        
        # ç»Ÿè®¡å„åˆ†ç±»çš„å®é™…å›¾ç‰‡æ•°é‡
        for category_dir in self.cells_dir.iterdir():
            if category_dir.is_dir():
                # ç»Ÿè®¡å„ç§ç±»å‹çš„æ–‡ä»¶
                jpg_count = len(list(category_dir.glob("*.jpg")))
                png_count = len(list(category_dir.glob("*.png")))
                gif_count = len(list(category_dir.glob("*.gif")))
                webp_count = len(list(category_dir.glob("*.webp")))
                total_count = jpg_count + png_count + gif_count + webp_count
                
                report["åˆ†ç±»ç»Ÿè®¡"][category_dir.name] = {
                    "æ€»æ•°": total_count,
                    "JPG": jpg_count,
                    "PNG": png_count,
                    "GIF": gif_count,
                    "WEBP": webp_count
                }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.cells_dir / "ä¸‹è½½æŠ¥å‘Š.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logging.info(f"ä¸‹è½½æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        print("\n" + "="*60)
        print("ğŸ”¬ äººä½“ç»†èƒå›¾ç‰‡ä¸‹è½½å®Œæˆ! ğŸ”¬")
        print(f"æ€»å…±ä¸‹è½½: {total_downloaded} å¼ å›¾ç‰‡")
        print("\nåˆ†ç±»ç»Ÿè®¡:")
        for category, stats in report["åˆ†ç±»ç»Ÿè®¡"].items():
            if stats["æ€»æ•°"] > 0:
                print(f"  ğŸ“ {category}: {stats['æ€»æ•°']} å¼ ")
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ äººä½“ç»†èƒå›¾ç‰‡çˆ¬è™« ğŸ”¬")
    print("ä¸“é—¨çˆ¬å–å„ç§äººä½“ç»†èƒçš„é«˜è´¨é‡å›¾ç‰‡ï¼ˆä½¿ç”¨è‹±æ–‡å…³é”®è¯ï¼‰")
    print("-" * 50)
    
    # è¯¢é—®ç”¨æˆ·é…ç½®
    print("\né…ç½®é€‰é¡¹:")
    print("1. æ¯ä¸ªç»†èƒç±»å‹ä¸‹è½½å¤šå°‘å¼ å›¾ç‰‡? (é»˜è®¤: 25)")
    max_images = input("è¯·è¾“å…¥æ•°å­— (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤å€¼): ").strip()
    
    try:
        max_images_per_category = int(max_images) if max_images else 25
    except ValueError:
        max_images_per_category = 25
        print("è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼: 25")
    
    print(f"\nå°†ä¸ºæ¯ä¸ªç»†èƒç±»å‹ä¸‹è½½æœ€å¤š {max_images_per_category} å¼ å›¾ç‰‡")
    print("æœç´¢æº: Bingã€Unsplashç­‰å›½å¤–ç½‘ç«™")
    print("å¼€å§‹ä¸‹è½½...\n")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶è¿è¡Œ
    crawler = CellImageCrawler()
    
    try:
        crawler.run(max_images_per_category=max_images_per_category)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        logging.error(f"ç¨‹åºå¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()